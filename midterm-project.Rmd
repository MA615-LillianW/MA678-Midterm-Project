---
title: "MA678 Midterm Project"
author: "Shicong Wang"
date: "12/10/2021"
output: pdf_document
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(
  "ggplot2",
  "knitr",
  "arm",
  "foreign",
  "faraway",
  "nnet",
  "VGAM",
  "MASS",
  "rstanarm",
  "magrittr",
  "dplyr",
  "reshape2",
  "stats",
  "tidyverse", 
  "rvest",
  "xts",
  "RColorBrewer",
  "tidyverse",
  "wordcloud2",
  "plotly",
  "forcats",
  "hrbrthemes",
  "viridisLite",
  "viridis",
  "ggridges",
  "GGally",
  "xts",
  "gridExtra",
  "lattice",
  "lm4",
  "performance",
  "see",
  "patchwork",
  "glmmTMB",
  "fmsb",
  "pals",
  "scales",
  "RColorBrewer"
  
)

```

## Abstract

Employment has always been a source of pressure for graduates. Everyone is eager to find the most suitable job for them, holding a satisfactory salary in a favorite company and handy position. However, graduates are often confused by complicated information. They need to compare salaries in multiple companies and positions, and they are also screened by the various requirements of different companies. As a result, making analysis in the factors effecting salaries attaches vital importance. Graduates are supposed to make sense so that they can try to meet the requirements before graduating, and given to their conditions choose the most suitable job after graduation.

## Introduction

The factors that determine salary are complex. It may be related to the employee's personal characteristics, such as the employee's education level, work experience, and even gender and race. Additionally, it is with respect to the company itself like the location of the company and the company types. Even in the same company, the salary of an employee is closely depended on his position and rank. Therefore, the analysis of factors affecting salary should be comprehensively considered from multiple aspects and perspectives. Although it seems natural that a PhD may worth higher salary than a master degree, or a experienced staff is more welcomed than a graduation. Nevertheless, when it comes to the comparison between the salaries of an Asian male graduation with a PhD in software engineering at Apple Inc and a white girl working five years with a master's degree in data science at Google, things can be confused.

As a result, a simple linear regression model cannot solve this problem containing various types of dependent variables. In the following steps, I will build a multilevel model to research the data set whose data for participants are organized at more than one level.  

## Method

### Data Cleaning and Processing

The data set I choose can be downloaded on [Kaggle: Data Science and STEM salaries  ](https://www.kaggle.com/jackogozaly/data-science-and-stem-salaries). The data set contains more that 62,000 STEM salary records from 1085 top companies all around the world, and involves useful information such as education level, compensation (base salary, bonus, stock grants), race, and more, which serve as the dependent variables in the model.

Since the data set came from a survey, the surveyors did not pay attention to the uniformity of the company name format when filling in the questionnaires, for instance, Jp Morgan showed in different format like "JpMorgan", "Jpmorgan", so firstly I tried to make sure that one company name is presented in one form. Secondly, I separated the columns with abundant information to guarantee that the information in the each column is in more details. Thirdly, I dropped the default values in the data set. Finally, I selected some columns and mutated them into new sub-data-sets to make comparison in different dimensions.
Here are some explanations of columns:

| column names             | explanations
| :--:                     | :----- |
| title                    | The Specific position in companies |
| totalyearlycompensation  | Cumulative value of one year's salary |
| level                    | The ranks within the companies |
| yearsofexperience        | How long is the staff works |
| yearsatcompany           | How long is the staff in this company |
| states                   | The states where the companies are located|
| edu_level                | Five levels according to acedemic degree|
| work_experience          | Four levels according to the years of experience|


```{r include=FALSE}
data_salary<- read.csv("Levels_Fyi_Salary_Data.csv")
data_salary$company<- tolower(data_salary$company)

trim <- function (x){
  gsub("^\\s+|\\s+$", "", x)
  }

trim(data_salary$company) 
trim(data_salary$country)
trim(data_salary$date)

# company
data_salary$company[data_salary$company == "amzon"] <- "amazon"
data_salary$company[data_salary$company == "apple inc."] <- "apple"
data_salary$company[data_salary$company == "aruba networks"] <- "aruba"
data_salary$company[data_salary$company == "bloomberg lp"] <- "bloomberg"
data_salary$company[data_salary$company == "booking"] <- "booking.com"
data_salary$company[data_salary$company == "cgi"] <- "cgi group"
data_salary$company[data_salary$company == "dish"] <- "dish network"
data_salary$company[data_salary$company == "intel corporation"] <- "intel"
data_salary$company[data_salary$company == "johnson & johnson"] <- "johnson and johnson"
data_salary$company[data_salary$company == "johnson"] <- "johnson and johnson"
data_salary$company[data_salary$company == "jp morgan chase"] <- "jpmorgan chase"
data_salary$company[data_salary$company == "jp morgan"] <- "jpmorgan"
data_salary$company[data_salary$company == "macy's,"] <- "macy's"
data_salary$company[data_salary$company == "microsoft corporation"] <- "microsoft"
data_salary$company[data_salary$company == "costco"] <- "costco wholesale"
data_salary$company[data_salary$company == "google llc"] <- "google"
data_salary$company[data_salary$company == "nxp"] <- "nxp semiconductors"
   
# divide location
data_salary %<>% separate(col=location,
                     into = c("city", "states", "country"), 
                     sep = ",",
                     fill = "right")
data_salary$country[is.na(data_salary$country)] <- "United States"
dat_usa<- data_salary %>% filter(country== "United States")

data_salary %<>% separate(col=timestamp,
                     into = c("date", "time"), 
                     sep = " ",
                     fill = "right")

# add education level
data_salary$edu_level <- ifelse(data_salary$Highschool == 1,0,
                                  ifelse(data_salary$Some_College == 1,1,
                                        ifelse(data_salary$Bachelors_Degree == 1,2,
                                               ifelse(data_salary$Masters_Degree == 1,3,
                                                      ifelse(data_salary$Doctorate_Degree == 1,4,0
                                                        )))))

# sub-data-set
dat4<- data_salary %>%
  count(company)
dat4[order(dat4$n,decreasing=T),]
dat_company<- data_salary %>% 
  filter(company == "amazon" | company == "microsoft" | company == "google" | company == "facebook" | company == "facebook" |company == "apple" | company == "oracle" | company == "salesforce" | company == "intel" | company == "ibm" | company == "cisco")
dat5<- dat_company %>%
  count(company,title,level)

# add race
df_1 <- data_salary %>% 
  na.omit() %>%
  filter(country == "United States")
df_1$gender[df_1$gender == "Male"]<- "1"
df_1$gender[df_1$gender == "Female"]<- "0"
df_1$gender<- as.numeric(df_1$gender)
df_1$race <- ifelse(df_1$Race_Asian == 1,1,
                                  ifelse(df_1$Race_White == 1,2,
                                        ifelse(df_1$Race_Two_Or_More == 1,3,
                                               ifelse(df_1$Race_Black == 1,4,
                                                      ifelse(df_1$Race_Hispanic == 1,5,0
                                                        )))))

```

### Exploratory Data Analysis

As the factors taking into consideration are multiple, it is unrealistic to present these factors in one picture, so I make efforts to draw a series of plots to show the effects of different factors on salary. 
Due to the large difference in magnitude among the variables, most of the points will accumulate at the bottom of the image. As a result, I use "log(totalyearlycompensation)" to substitute "totalyearlycompensation". Maybe the image is not as intuitive as before, but the distribution of points can be seen more clearly.

Here some plots to see if there is correlation among job titles and companies with total yearly compensations.

```{r,echo=FALSE,warning=FALSE,fig.height=4,fig.width= 6,fig.cap="Make comparison among education level and work experience"}
# boxplot(salary, years of experience, education level)
data_salary$work_experience <- ifelse(data_salary$yearsofexperience <= 1,"Graduates",
                                  ifelse(data_salary$yearsofexperience <= 5,"Novices",
                                        ifelse(data_salary$yearsofexperience <= 10,"intermediates",
                                               ifelse(data_salary$yearsofexperience >10,"Experienced","grades"                                                                                                                  ))))
df_2 <- data_salary %>% 
  na.omit() %>%
  filter(country == "United States")
df_2$logvalue<- log(df_2$totalyearlycompensation)
mean<- mean(df_2$logvalue)
ggplot(df_2, aes(x =Education, y = log(totalyearlycompensation), color = work_experience)) +  # ggplot function
  geom_boxplot()+
  xlab("Education Level") +
  ylab("log total yearly experience") +
  geom_hline(aes(yintercept=mean),colour="blue", linetype = "twodash",size=1)+
  theme(legend.position ="top",axis.text.x=element_text(angle=30))
```

The first plot shows the correlations among total yearly compensation, years of experience and education level. The result was unexpected. Education level and years of work experience affect the salary level to a certain extent, but not exactly as we imagined. Generally speaking, a Ph.D. has a significant advantage in salary. For other education level, perhaps a higher degree has certain advantages when first enter the job, but with the accumulation of work experience, such advantages become less obvious. Of course, more work experience does mean more salaries in this plot. As is shown in the plot that the blue line represents mean value, it's obvious that majority of graduates obtain salaries below average.
Also, I make comparisons of total yearly compensation among race and gender.To relief, neither of these factors have a significant impact on salary.It can be seen that male employees have certain advantages in the high-income range, but generally speaking, the median and mode of male and female employees’ income are not much different. The plot is attached in appendix.

```{r echo=FALSE,warning=FALSE,fig.height=3,fig.width=6,fig.cap="Make comparison among job titles"}
# salary and title(volin plot)
dat3<- data_salary %>%
  count(company,title,level,Education)
data_salary %>% ggplot(aes(x =log(totalyearlycompensation), y =  title, fill = title)) +
  geom_density_ridges() +
   scale_fill_viridis(discrete=TRUE) +
   scale_color_viridis(discrete=TRUE) +
  theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8))
```

```{r echo=FALSE,warning=FALSE,fig.height=3,fig.width=5,fig.cap="Make comparison among compaines"}
# salary and company
dat_company %>%
  ggplot(aes(y=company, x=log(totalyearlycompensation), fill=company)) +
    geom_density_ridges(alpha=0.6, stat="binline", bins=20) +
    xlab("log total yearly compensation") +
    ylab("company")+
   theme(
      legend.position="none",
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    )

```

The plots above respectively show the relationship between job titles, companies and total yearly compensations. Apparently, both of them exert effect on total yearly compensations. When it comes to job titles, software engineering manager and product manager seems enjoy better salary treatments. Besides, when it comes to companies, Google, Facebook and Salesforce obtain higher employees' salaries, whereas IBM's salaries are less attractive. When analyze staff salaries are related to which factors, both can be used as the basis for grouping staff.


```{r echo=FALSE,warning=FALSE,fig.height=4, fig.width=6, fig.cap="correlation plot of variables"}
library(ggcorrplot)
data_num<- df_1[,c(10,11,13,14,15,33,34)]
data_num$basesalary<- log(data_num$basesalary)
data_num$bonus<- log(data_num$bonus+1)
data_num$stockgrantvalue<- log(data_num$stockgrantvalue+1)
corr <- cor(data_num)
ggcorrplot(corr,
  hc.order = TRUE, type = "lower",
  outline.color = "white",
  ggtheme = ggplot2::theme_gray,
  colors = c("#6D9EC1", "white", "#E46726")
)
```

Finally, make preliminary preparations for the establishment of the model. I use correlation plot to observe the correlation between the selected independent variables. The color depth reflects the size of the correlation, where red represents a positive correlation, and blue represents a negative correlation. As is seen in the plot, the factors above al have a certain impact on salary. Since the number of company in the data set is 1085, which is too large for grouping. However, the 21 most frequent companies contain most of the data in the data set, so companies can be used as the basis for grouping. 

### Model Fitting

In this section, use multilevel model to make analysis on which factors will exert influence on salary. Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level. Given on the information and conclusions above, taking years of experience, years at company, education level, gender, race into considerations are proper. Next, we need to build a model to group all the data in the data set according to the company, and calculate the coefficients of each group through the model. By comparing these coefficients, we can draw some more robust conclusions: for example, which company’s data science jobs have higher salaries.

For precise analysis, I select a sub-dataset which is consist of 21 most frequent companies.
And to see the fixed effects below, all variables are significant at alpha = 0.05 level.

|                     |Estimate   |Std. Error  |df        |t value  |Pr(>&#124;t&#124;) |
|:---:                |:---:      |:---:       |:---:     |:---:    |:---:              |
|(Intercept)          |11.48407   |0.06430     |16.75     |136.28   |5.28e-12 ***       |
|yearsofexperience    |0.03621    |0.00351     |24.53     |15.713   |6.21e-18 ***       |
|yearsatcompany       |0.00722    |0.00752     |12.21     |13.214   |2.09e-05 ***       |
|gender               |0.06030    |0.01336     |10.98     |5.083    |2.27e-07 ***       |
|race                 |-0.01380   |0.00654     |10.98     |-5.033   |2.31e-06 ***       |
|edu_level            |0.09008    |0.01092     |10.98     |6.096    |3.15e-08 ***       |

\newpage
## Result

### Model Coefficients

Given the model fit above, we can conclude this formula: 
$$y= 11.484 + 0.036x_1 - 0.007x_2 + 0.060x_3 - 0.014x_4   + 0.090x_5$$
Let $x1=yearsofexperoenve$, $x2=yearsatcompany$, $x3=gender$, $x4=gender$, $x5=education level$, 
$y=log(totalyearlycompensation)$.


Since the magnitude value of total yearly compensation, in order to avoid the coefficient of the regression formula being too large, I use log instead. The education level and ethnicity are assigned separately and added as independent variables to the regression model.So the formula contains two continuous variables and three discrete variables.

As for various job title in different companies, the degree of influence on each independent variable is different.I choose the representative job titles below: 


|Company           |(Intercept) |Data Scientist    |Software Engineer       |Sales       |Product Designer
|:---:             |:---:       |:---:             |:---:                   |:---:       |:---:
|Amazon            |11.50       |0.380             |0.386                   |0.384       |0.316
|Apple             |11.74       |0.072             |0.185                   |-0.600      |0.227
|Facebook          |11.66       |0.303             |0.481                   |-0.112      |0.358
|Google            |11.71       |0.186             |0.273                   |0.068       |0.255
|Jpmorgan chase    |11.20       |0.217             |0.173                   |0.326       |0.217
|Microsoft         |11.65       |0.226             |0.159                   |0.278       |0.194

Here are the brief interpretation of the coefficients above: These coefficients reflect to some extent the company's demand for these job titles. The higher the coefficient, the greater the positive impact of these job titles on salary, and it also shows that the company is more willing to pay higher salaries for these job titles. I select 6 top companies and 4 representative job titles to make comparison. Take Apple Inc as example: From the coefficient in the table, Apple attach more importance on product designer than other job titles, which is reasonable that Apple launches new products every year, and updates and enhancements in the previously more mature versions require broad forward-looking and in-depth understanding of the market for the product design team.
If we focus on Data Scientist, it seems that it is well-demanded in most companies in the form, in which Amazon and Facebook can be the better choice since the higher coefficient, however the result depends cause we also need to combine the basesalary in each company to make more ideal options.
When it comes to the comparison among job titles and companies, using radar plot can be more intuitive. The plots are contained in appendix.

### Model validation

Model validation is the task of confirming the outputs of a statistical model have enough fidelity to the outputs of the data-generating process that the objectives of the investigation can be achieved. In this section, I use the function "check_model" in package "performance" to realize it. The plots are shown in appendix, respectively reflect diverse aspects of fitting models.
In the linearity and homogeneity of variance part, the reference lines are flat and horizontal. For the collinearity plot, the higher bar(>5) indicate potential collinearity issues, and in this plot all the bars are below that. When it comes to influential observations, points are inside the contour line. And in normality of residuals, dots fall along the line, meaning the distribution is close to the normal curve. So are the dots in normality of random effects plot. As a result, the model does make sense.


## Discussion

In a sense, this model is reasonable.Relatively speaking, a person with many years of work experience and a higher degree is indeed more likely to get a higher salary. At the same time, in the sample of this data set, men are slightly better than women in both the number and salary in the STEM field. But this does not mean that women will be inferior to men in the field of STEM.And in different companies, these factors do have different effects, that's why we use multilevel model to make this analysis.

In fact, there are many factors that can be taken into consideration in this analysis. For example, as is mentioned above, cause there are 1085 companies in the table, I select 21 companies contained majority of data instead. Nevertheless, if the companies can be grouped into types, like tech company, investment company, service company etc., we can compare salaries of companies in same type or distinct types.At the same time, analyzing correlation of the salary and state, it can be found that the salary level of each state will also be different. However, there are too many factors involved in the salary difference between state and state, such as policy factors, economic development level, etc., and it is difficult to conduct further analysis only through the factors in the table.

\newpage
# Appendix

## More EDA

### Redar Plot

```{r,echo=FALSE, fig.height=5, fig.width=8,warning=FALSE}
df_6<- df_1 %>% filter(title=="Data Scientist" | title=="Software Engineer" |title== "Sales" | title=="Product Designer")
#df_8<- df_6 %>% filter(title=="Sales")
#table(df_8$yearsofexperience)
df_7<- df_1 %>% filter(company=="amazon"|company=="microsoft"|company=="apple"|company=="facebook"|company=="google"|company=="jpmorgan chase")
radar_data1 <- df_6 %>% group_by(title) %>% dplyr::summarise(
  edu_level = mean(edu_level), race=mean(race),
  yearsofexperience = mean(yearsofexperience), yearsatcompany = mean(yearsatcompany), 
  totalyearlycompensation = mean(totalyearlycompensation)
)

radar_data2 <- df_7 %>% group_by(company) %>% dplyr::summarise(
  edu_level = mean(edu_level), race=mean(race),
  yearsofexperience = mean(yearsofexperience), yearsatcompany = mean(yearsatcompany), 
  totalyearlycompensation = mean(totalyearlycompensation)
)
# To use the fmsb package, I have to add 2 lines to the dataframe: the max and min of each variable to show on the plot!

# Set graphic colors
coul <- brewer.pal(3, "BuPu")
colors_border <- coul
colors_in <- alpha(coul,0.4)
rownames(radar_data1) <- radar_data1$title
rownames(radar_data2) <- radar_data2$company
```

```{r,echo=FALSE, fig.height=5, warning=FALSE,fig.width=8,warning=FALSE}
radarchart(radar_data1[,-1], axistype=1 , maxmin=F,
    #custom polygon
    pcol= colors_border , pfcol=colors_in , plwd=1 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="black", cglwd=1, 
    #custom labels
    vlcex=0.8 
    )
legend(x= 0.7, y= 1.4, legend = rownames(radar_data1), bty = "n", pch=15, col=colors_in , text.col = "grey", cex=0.8, pt.cex=0.8)
```

```{r,echo=FALSE, fig.height=5, fig.width=8,warning=FALSE}
radarchart(radar_data2[,-1], axistype=1 , maxmin=F,
    #custom polygon
    pcol= colors_border , pfcol=colors_in , plwd=1 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="black", cglwd=1, 
    #custom labels
    vlcex=0.8 
    )
legend(x= 0.7, y= 1.4, legend = rownames(radar_data2), bty = "n", pch=10, col=colors_in , text.col = "grey", cex=0.8, pt.cex=0.8)

```

### violin plot

```{r,echo=FALSE,warning=FALSE,fig.height=4,fig.width=6}
# violin plot(salary, gender and race)
ggplot(df_2,aes(fill=gender, y=log(totalyearlycompensation), x=Race)) + 
    geom_violin(position="dodge", alpha=0.5, outlier.colour="transparent") +
    scale_fill_viridis(discrete=T, name="") +
    xlab("race") +
    ylab("log total yearly experience") +
    geom_hline(aes(yintercept=mean),colour="blue", linetype = "twodash",size=1)+
  theme(legend.position = "top")
```

### Density plot

```{r echo=FALSE,warning=FALSE,fig.height=5, fig.width= 10}
#dat1<- data_salary %>% count(company)
#wordcloud2(dat1,size=3,color='random-dark')
# Density plot
## years of experience distribution
ggplot(data_salary, aes(x=yearsofexperience))+
  geom_histogram(aes(y=..density..), color="black", fill="white")+
  geom_density(alpha=.2,fill="#FF6666") +
    xlab("years of experience") +
    ylab("density") 

## total yearly compensation distribution
ggplot(data_salary, aes(x=totalyearlycompensation))+
  geom_histogram(aes(y=..density..), color="black", fill="white")+
  geom_density(alpha=.2,fill="#69b3a2") +
    xlab("total yearly compensation") +
    ylab("density") 

## education level distribution
df_2 <- data_salary %>% 
  na.omit() %>%
  filter(country == "United States")
ggplot(data=df_2, aes(x=log(totalyearlycompensation),group=Education, fill = Education))+
  geom_histogram(aes(y=..density..))+
  geom_density(adjust=1.5,alpha=.2)+
    #theme_ipsum()+
    facet_wrap(~Education)+
    xlab("total yearly compensation") +
    ylab("density") 
```

### Lollipop (Count of company in each state )

```{r echo=FALSE,warning=FALSE,fig.height=7, fig.width=15}
## company in states distribution
dat6<- dat_usa %>% count(states)
ggplot(dat6, aes(x=states, y=log(n)))+
  geom_segment( aes(x=states, xend=states, y=0, yend=log(n)), color="skyblue") +
  geom_point( color="blue", size=4, alpha=0.6) +
  #theme_light() +
  coord_flip() +
    xlab("States") +
    ylab("log frequency") 
```

### Circular Bar Plot( Distribution of job title in each educaiton level)

```{r echo=FALSE,warning=FALSE,fig.height=6, fig.width= 15}
## compare job title and education level
data_title<- df_2 %>% count(Education,title)
data<- data_title
empty_bar <- 5
to_add <- data.frame( matrix(NA, empty_bar*nlevels(data$Education), ncol(data)) )
colnames(to_add) <- colnames(data)
to_add$group <- rep(levels(data$Education), each=empty_bar)
data <- rbind(data, to_add)
data <- data %>% arrange(Education)
data$id <- seq(1, nrow(data))
 
# Get the name and the y position of each label
label_data <- data
number_of_bar <- nrow(label_data)
angle <- 90 - 360 * (label_data$id-0.5) /number_of_bar     # I substract 0.5 because the letter must have the angle of the center of the bars. Not extreme right(1) or extreme left (0)
label_data$hjust <- ifelse( angle < -90, 1, 0)
label_data$angle <- ifelse(angle < -90, angle+180, angle)
 
# Make the plot
p <- ggplot(data, aes(x=as.factor(id), y=n, fill=Education)) +       # Note that id is a factor. If x is numeric, there is some space between the first bar
  geom_bar(stat="identity", alpha=0.5) +
  ylim(-100,120) +
  coord_polar() + 
  geom_text(data=label_data, aes(x=id, y=n+10, label=title, hjust=hjust), color="black", fontface="bold",alpha=0.6, size=2.5, angle= label_data$angle, inherit.aes = FALSE ) 
 
p+theme_minimal() +
  theme(
    #legend.position = "none",
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.margin = unit(rep(-1,4), "cm"))
  
```

\newpage
### Full Results

```{r echo=FALSE,warning=FALSE}
df_1 <- data_salary %>% 
  na.omit() %>%
  filter(country == "United States")
df_1$gender[df_1$gender == "Male"]<- "1"
df_1$gender[df_1$gender == "Female"]<- "0"
df_1$gender<- as.numeric(df_1$gender)

df_1$race <- ifelse(df_1$Race_Asian == 1,1,
                                  ifelse(df_1$Race_White == 1,2,
                                        ifelse(df_1$Race_Two_Or_More == 1,3,
                                               ifelse(df_1$Race_Black == 1,4,
                                                      ifelse(df_1$Race_Hispanic == 1,5,0
                                                        )))))
company_counts <- df_1 %>%
    group_by(company) %>%
    tally
# get names of the company with counts > 120
frequent_company <-  company_counts %>%
    filter(n >= 120) %>%
    select(company)
# filter out the most-frequent companies
df_3 <- df_1 %>%
    filter(company %in% frequent_company$company)
model2<- stan_lmer(log(totalyearlycompensation)~ yearsofexperience + yearsatcompany + gender + race + edu_level + (1+yearsofexperience+gender+race+edu_level+yearsatcompany+title|company), data=df_3, refresh=0)

```

### model checking
```{r,echo=FALSE,fig.height=20,fig.width=25,warning=FALSE}
check_model(model2)
```

### Randomc effects of model
```{r,echo=FALSE,warning=FALSE}
ranef(model2)
```

### Fixed effects of model
```{r,echo=FALSE,warning=FALSE}
fixef(model2)
```

### Coefficients of model
```{r,echo=FALSE,warning=FALSE}
coef(model2)
```


